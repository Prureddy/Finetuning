{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üìñ Notebook Overview & Unsloth Primer\n",
        "\n",
        "**Purpose:**  \n",
        "This notebook demonstrates instruction-style fine-tuning of a 7B Mistral model using the Unsloth library‚Äôs 4-bit quantization and optimized LoRA adapters, powered by TRL‚Äôs `SFTTrainer`. You will learn how to:\n",
        "\n",
        "1. **Leverage 4-bit Quantization:**  \n",
        "   - Drastically reduce VRAM usage (‚àº4√ó smaller weights) while retaining full inference quality.  \n",
        "   - Enable training of large models on a single GPU.\n",
        "\n",
        "2. **Use Unsloth‚Äôs Gradient Checkpointing & Quant LoRA:**  \n",
        "   - **Gradient checkpointing (‚Äúunsloth‚Äù mode):** trades a bit of compute for memory, allowing longer contexts or larger batch sizes.  \n",
        "   - **LoRA adapters + RSLoRA support:** inject tiny trainable rank-decomposed matrices into attention (q/k/v/o, gate/up/down projections) for parameter efficiency.\n",
        "\n",
        "3. **Integrate with TRL‚Äôs SFTTrainer:**  \n",
        "   - Simplifies instruction-tuning with built-in support for text-only fields.  \n",
        "   - Packs or streams examples, logs training metrics, and handles mixed-precision seamlessly.\n",
        "\n",
        "---\n",
        "\n",
        "## üîë Key Concepts\n",
        "\n",
        "- **4-bit Quantization (`load_in_4bit=True`):**  \n",
        "  - Reduces model weight storage & memory bandwidth.  \n",
        "  - Works with both GPT-style and LLaMA-style architectures via BNB backend.\n",
        "\n",
        "- **Unsloth PEFT (`get_peft_model`):**  \n",
        "  - Combines LoRA with Unsloth‚Äôs ‚Äúunsloth‚Äù checkpointing to save ‚àº30 % VRAM.  \n",
        "  - Optional ‚Äúrslora‚Äù for rank-stabilized updates.\n",
        "\n",
        "- **Instruction Format:**  \n",
        "  - Uses Alpaca-style prompts (instruction + input ‚Üí response) or chat templates for downstream tasks.\n",
        "\n",
        "---\n",
        "\n",
        "## üóÇ Notebook Workflow\n",
        "\n",
        "1. **Install & Verify**  \n",
        "   Install Unsloth nightly build, Transformers, PEFT, TRL, and Torch. Verify versions.\n",
        "\n",
        "2. **Imports & Env Setup**  \n",
        "   All Python imports, environment variables, and CUDA checks in one place.\n",
        "\n",
        "3. **GPU & CUDA Check**  \n",
        "   Confirm torch CUDA availability, device count/name, and `nvcc --version`.\n",
        "\n",
        "4. **UnslothLoader Class**  \n",
        "   Encapsulate model/tokenizer loading with 4-bit support and sequence length configuration.\n",
        "\n",
        "5. **PEFT Configuration**  \n",
        "   Wrap the base model with LoRA adapters, Unsloth checkpointing, and optional RSLoRA.\n",
        "\n",
        "6. **Dataset Preparation**  \n",
        "   Load Hugging Face data, rename columns, and format examples in Alpaca/chat style.\n",
        "\n",
        "7. **SFTTrainer Setup**  \n",
        "   Build TRL‚Äôs `SFTTrainer` with `TrainingArguments` tuned for quantized LoRA training.\n",
        "\n",
        "8. **Training & Monitoring**  \n",
        "   Log baseline and peak GPU memory reservation; run `.train()` to fine-tune.\n",
        "\n",
        "9. **Save Checkpoint**  \n",
        "   Merge LoRA adapters back into the base model and save both model + tokenizer.\n",
        "\n",
        "10. **Inference Examples**  \n",
        "    Reload the merged 4-bit model, enable Unsloth inference optimizations, and generate sample outputs with streaming.\n",
        "\n",
        "---\n",
        "\n",
        "## üí° Deep Tips & Best Practices\n",
        "\n",
        "- **Batch Size vs. Sequence Length:**  \n",
        "  Quantization + checkpointing lets you trade sequence length for batch size‚Äîexperiment to find your sweet spot.\n",
        "\n",
        "- **Mixed Precision:**  \n",
        "  Prefer `torch.float16` on Ampere+ GPUs. Use `is_bfloat16_supported()` to conditionally enable BF16.\n",
        "\n",
        "- **Logging & Debugging:**  \n",
        "  - Set `logging_steps=1` for quick feedback in small demos.  \n",
        "  - Watch out for OOM‚Äîreduce `per_device_train_batch_size` or disable packing.\n",
        "\n",
        "- **Reproducibility:**  \n",
        "  Always set `random_state` in Unsloth and `seed` in `TrainingArguments` to ensure consistent runs.\n",
        "\n",
        "---\n",
        "\n",
        "> **Next:** proceed to **Cell 1: Install Unsloth & Core Dependencies** and get your environment ready!\n"
      ],
      "metadata": {
        "id": "Q2pFOlunCQrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#üì¶ Install Unsloth & Core Dependencies\n",
        "- Single cell for all pip installs and upgrades."
      ],
      "metadata": {
        "id": "9V-3mpGRBMLy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JWHDtPZBEmr"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "# Install Unsloth core + nightly, transformers, tokenizers, PEFT, Torch\n",
        "pip install --upgrade --no-cache-dir unsloth tokenizer torch peft \\\n",
        "    && pip uninstall unsloth -y \\\n",
        "    && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
        "\n",
        "# Verify key packages\n",
        "pip show torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#üì• Imports & Environment Configuration\n",
        "- Group all imports and any os/env setup here."
      ],
      "metadata": {
        "id": "wPINvkkVBSgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, os\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import AutoTokenizer, TextStreamer\n",
        "from datasets import load_dataset\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "# (Optional) show Python executable\n",
        "print(\"Python executable:\", sys.executable)\n"
      ],
      "metadata": {
        "id": "h6tYjcXSBRoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#üñ•Ô∏è GPU Check & CUDA Toolkit Verification\n",
        "- Confirm GPU availability"
      ],
      "metadata": {
        "id": "e0jbHK5oBXxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_gpu():\n",
        "    \"\"\"Print GPU count, names, and availability.\"\"\"\n",
        "    print(\"CUDA available:\", torch.cuda.is_available())\n",
        "    print(\"Device count:\", torch.cuda.device_count())\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"Device name:\", torch.cuda.get_device_name(0))\n",
        "    else:\n",
        "        print(\"No GPU found.\")\n",
        "\n",
        "check_gpu()\n",
        "\n"
      ],
      "metadata": {
        "id": "ZPpYcFPjBeFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#üõ†Ô∏è Unsloth Model Loader Class\n",
        "- Encapsulate model + tokenizer loading with 4-bit support."
      ],
      "metadata": {
        "id": "px7uxzogBhpb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UnslothLoader:\n",
        "    \"\"\"\n",
        "    Loads an UnsLo¬≠th-supported LLM in 4-bit or full precision,\n",
        "    sets max sequence length and dtype automatically.\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name: str, max_seq: int = 2048, dtype=None, load4bit: bool = True):\n",
        "        self.model_name = model_name\n",
        "        self.max_seq = max_seq\n",
        "        self.dtype = dtype\n",
        "        self.load4bit = load4bit\n",
        "        self.model, self.tokenizer = None, None\n",
        "\n",
        "    def load(self):\n",
        "        \"\"\"Download and instantiate model + tokenizer.\"\"\"\n",
        "        self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
        "            model_name=self.model_name,\n",
        "            max_seq_length=self.max_seq,\n",
        "            dtype=self.dtype,\n",
        "            load_in_4bit=self.load4bit\n",
        "        )\n",
        "        print(f\"Loaded {self.model_name} with 4-bit={self.load4bit}\")\n",
        "\n",
        "# Example usage:\n",
        "loader = UnslothLoader(\"unsloth/mistral-7b-v0.2-bnb-4bit\")\n",
        "loader.load()\n",
        "model, tokenizer = loader.model, loader.tokenizer\n"
      ],
      "metadata": {
        "id": "TN8YPS9iBk3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üîß Apply LoRA & Unsloth Optimizations\n",
        "- Wrap the base model with PEFT adapters + Unsloth checkpointing."
      ],
      "metadata": {
        "id": "f-gXgIUqBnDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def configure_peft(model, r=16, alpha=16, dropout=0.0, use_rslora=False):\n",
        "    \"\"\"\n",
        "    Inject LoRA adapters and Unsloth-specific settings for VRAM efficiency.\n",
        "    \"\"\"\n",
        "    model = FastLanguageModel.get_peft_model(\n",
        "        model,\n",
        "        r=r,\n",
        "        target_modules=[\n",
        "            \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\n",
        "            \"gate_proj\",\"up_proj\",\"down_proj\"\n",
        "        ],\n",
        "        lora_alpha=alpha,\n",
        "        lora_dropout=dropout,\n",
        "        bias=\"none\",\n",
        "        use_gradient_checkpointing=\"unsloth\",\n",
        "        random_state=3407,\n",
        "        use_rslora=use_rslora,\n",
        "        loftq_config=None\n",
        "    )\n",
        "    print(\"PEFT + Unsloth configured!\")\n",
        "    return model\n",
        "\n",
        "# Apply\n",
        "model = configure_peft(model)\n"
      ],
      "metadata": {
        "id": "BcBSY_hTBqJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#üîÑ Dataset Prep & Prompt Formatting\n",
        "- Load raw data, rename columns, and format in Alpaca/chat style."
      ],
      "metadata": {
        "id": "PqGgmaUnBroF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Load and rename\n",
        "ds = load_dataset(\"KingNish/reasoning-base-20k\", split=\"train\")\n",
        "ds = ds.rename_columns({\"user\":\"instruction\",\"reasoning\":\"input\",\"assistant\":\"output\"})\n",
        "\n",
        "# 2) Alpaca-style formatting\n",
        "ALPACA_TMPL = (\n",
        "    \"Below is an instruction with context. Write an appropriate response.\\n\\n\"\n",
        "    \"### Instruction:\\n{}\\n\\n### Input:\\n{}\\n\\n### Response:\\n{}\"\n",
        ")\n",
        "\n",
        "def format_alpaca(ex):\n",
        "    text = [\n",
        "        ALPACA_TMPL.format(i, inp, out) + tokenizer.eos_token\n",
        "        for i, inp, out in zip(ex[\"instruction\"], ex[\"input\"], ex[\"output\"])\n",
        "    ]\n",
        "    return {\"text\": text}\n",
        "\n",
        "ds = ds.map(format_alpaca, batched=True)\n",
        "print(\"Sample formatted prompt:\\n\", ds[0][\"text\"][:200])\n"
      ],
      "metadata": {
        "id": "sXyptaZ_BumX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ Training Setup with SFTTrainer\n",
        "- Define and initialize the SFTTrainer for fine-tuning."
      ],
      "metadata": {
        "id": "Nhjw1pzcBv6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_sft_trainer(model, tokenizer, dataset, out_dir=\"unsloth_outputs\"):\n",
        "    \"\"\"\n",
        "    Prepare TRL's SFTTrainer for instruction-style fine-tuning.\n",
        "    \"\"\"\n",
        "    args = TrainingArguments(\n",
        "        output_dir=out_dir,\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        max_steps=60,             # placeholder for demo; use epochs in prod\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset=dataset,\n",
        "        dataset_text_field=\"text\",\n",
        "        max_seq_length=2048,\n",
        "        packing=False,\n",
        "        args=args\n",
        "    )\n",
        "    print(\"SFTTrainer ready.\")\n",
        "    return trainer\n",
        "\n",
        "trainer = build_sft_trainer(model, tokenizer, ds)\n"
      ],
      "metadata": {
        "id": "87DG9JoXBzKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìä Monitor GPU Memory & Start Training\n",
        "- Log baseline GPU memory, run .train(), then compute usage."
      ],
      "metadata": {
        "id": "EephA6qVB06Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline memory\n",
        "start_mem = torch.cuda.max_memory_reserved() / (1024**3)\n",
        "total_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
        "print(f\"Start reserved: {start_mem:.2f} GB / {total_mem:.2f} GB\")\n",
        "\n",
        "# Train\n",
        "train_stats = trainer.train()\n",
        "\n",
        "# Peak memory\n",
        "peak_mem = torch.cuda.max_memory_reserved() / (1024**3)\n",
        "print(f\"Peak reserved: {peak_mem:.2f} GB ({peak_mem/total_mem*100:.1f}%)\")\n"
      ],
      "metadata": {
        "id": "YjyF97M-B4aW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üíæ Save Fine-Tuned Model\n",
        "- Persist adapters and full model for inference."
      ],
      "metadata": {
        "id": "htPWACNlB6W3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(trainer, path=\"unsloth_finetuned\"):\n",
        "    \"\"\"\n",
        "    Saves LoRA adapters + base model merged into one checkpoint.\n",
        "    \"\"\"\n",
        "    model_merged = trainer.model.merge_and_unload()\n",
        "    model_merged.save_pretrained(path)\n",
        "    trainer.tokenizer.save_pretrained(path)\n",
        "    print(\"Model saved to:\", path)\n",
        "\n",
        "save_model(trainer)\n"
      ],
      "metadata": {
        "id": "bKdSITH1B9XY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß™ Inference Examples\n",
        "- Load the merged checkpoint and generate a sample response."
      ],
      "metadata": {
        "id": "lE4Pcdw6B-pG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload for inference\n",
        "inf_model, inf_tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth_finetuned\",\n",
        "    load_in_4bit=True\n",
        ")\n",
        "FastLanguageModel.for_inference(inf_model)\n",
        "\n",
        "def infer(prompt, max_new_tokens=100):\n",
        "    \"\"\"Generate text with streaming for real-time output.\"\"\"\n",
        "    inputs = inf_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    streamer = TextStreamer(inf_tokenizer)\n",
        "    inf_model.eval()\n",
        "    with torch.no_grad():\n",
        "        inf_model.generate(\n",
        "            **inputs,\n",
        "            streamer=streamer,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=0.3,\n",
        "            top_k=10,\n",
        "            eos_token_id=inf_tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "# Example\n",
        "sample_prompt = \"Prove that the difference between two consecutive cubes cannot be divisible by 5.\"\n",
        "infer(sample_prompt)\n"
      ],
      "metadata": {
        "id": "molssTguCBZS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}