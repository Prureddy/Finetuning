{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üìñ Notebook Overview & DeepSpeed Primer\n",
        "\n",
        "**Purpose:**  \n",
        "This notebook demonstrates fine-tuning Meta‚Äôs LLaMA-3.2-1B model with parameter-efficient LoRA adapters, accelerated by DeepSpeed‚Äôs ZeRO optimizations. You will learn:\n",
        "\n",
        "1. **Why DeepSpeed?**  Advanced memory and compute optimizations to:\n",
        "   - **Scale to large models** on limited GPUs by partitioning optimizer states and gradients (ZeRO Stages 1‚Äì3).  \n",
        "   - **Speed up training** with kernel fusion, sparse attention, and activation checkpointing.  \n",
        "   - **Simplify multi-GPU & distributed setups** via a unified API.\n",
        "\n",
        "2. **Key Concepts:**\n",
        "   - **ZeRO-Offload & ZeRO Stages**  \n",
        "     - Stage 1: partitions optimizer state.  \n",
        "     - Stage 2: also partitions gradients.  \n",
        "     - Stage 3: additionally partitions model weights.  \n",
        "   - **LoRA (Low-Rank Adaptation):** injects small, trainable adapter matrices into attention layers rather than updating all model weights.  \n",
        "   - **DeepSpeed Config:** JSON file (`config.json`) specifying ZeRO stage, offload settings (CPU/GPU), fp16, gradient accumulation, etc.\n",
        "\n",
        "3. **Notebook Workflow:**\n",
        "   1. **Install & Verify Dependencies:** PyTorch, Transformers, DeepSpeed, PEFT, etc.  \n",
        "   2. **Set Environment & Paths:** ensure CUDA/DeepSpeed pick up correct toolkit location.  \n",
        "   3. **Authenticate:** login to Hugging Face Hub for dataset/model access.  \n",
        "   4. **Load Model + Tokenizer:** configure padding tokens, dtype, device map.  \n",
        "   5. **Wrap with LoRA:** inject adapters for parameter-efficient fine-tuning.  \n",
        "   6. **Preprocess Dataset:** tokenize raw data, mask labels so loss only on generated tokens.  \n",
        "   7. **Initialize Trainer with DeepSpeed:** use `TrainingArguments(deepspeed=\"config.json\")` to automatically enable ZeRO.  \n",
        "   8. **Train & Monitor:** DeepSpeed handles gradient partitioning, offload, and logging.  \n",
        "   9. **Merge Adapters & Save:** consolidate LoRA weights into base model for inference portability.  \n",
        "   10. **Sample Inference:** load the merged model and generate a test response.\n",
        "\n",
        "4. **Deep Explanations & Tips:**\n",
        "   - **Memory Savings:**  \n",
        "     DeepSpeed ZeRO Stage 2 splits optimizer and gradient states across all GPUs; you get ~3√ó more effective memory than naive data-parallel.  \n",
        "   - **Speed vs. Precision Trade-offs:**  \n",
        "     Use `fp16=True` or `bf16=True` to cut memory in half, but watch out for numeric stability ‚Äî DeepSpeed includes ‚Äúloss scaling‚Äù to help.  \n",
        "   - **Configuration Highlights (`config.json`):**  \n",
        "     ```json\n",
        "     {\n",
        "       \"zero_optimization\": {\n",
        "         \"stage\": 2,\n",
        "         \"offload_param\": { \"device\": \"cpu\", \"pin_memory\": true },\n",
        "         \"offload_optimizer\": { \"device\": \"cpu\", \"pin_memory\": true }\n",
        "       },\n",
        "       \"fp16\": { \"enabled\": true, \"loss_scale\": 0 },\n",
        "       \"gradient_accumulation_steps\": 8,\n",
        "       \"train_micro_batch_size_per_gpu\": 2\n",
        "     }\n",
        "     ```  \n",
        "     - **offload_param** & **offload_optimizer**: push large tensors to CPU to fit in GPU memory.  \n",
        "     - **loss_scale = 0**: automatic dynamic scaling to avoid underflow in fp16.  \n",
        "   - **Debugging & Logs:**  \n",
        "     Enable `logging_steps` in `TrainingArguments` to get per-step loss. Look at the DeepSpeed console banner at startup for memory and throughput stats.\n",
        "\n",
        "---\n",
        "\n",
        "> **Next:** start with **Cell 1: Environment & Dependency Installation** to get your GPU and DeepSpeed ready!\n"
      ],
      "metadata": {
        "id": "1dMy9PdX_lqb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üì¶Environment & Dependency Installation\n",
        "- Install all required Python packages in one place."
      ],
      "metadata": {
        "id": "9KnZuucx9jLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install core libraries: PyTorch, Transformers, Datasets, PEFT, DeepSpeed\n",
        "!pip install torch transformers datasets peft deepspeed \\\n",
        "    && pip install mpi4py \\\n",
        "    && pip install torch --upgrade\n",
        "\n",
        "# Verify PyTorch installation\n",
        "!pip show torch\n"
      ],
      "metadata": {
        "id": "9abO2yNc9k9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üì• Imports & Path Configuration\n",
        "- Centralize all imports and basic OS path settings."
      ],
      "metadata": {
        "id": "-1MERzIX9qqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard library\n",
        "import os\n",
        "import importlib.metadata\n",
        "\n",
        "# Deep learning frameworks\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM,\n",
        "    TrainingArguments, Trainer\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Ensure CUDA toolkit path is set\n",
        "os.environ[\"CUDA_HOME\"] = \"/usr\"\n",
        "os.environ[\"PATH\"] = f\"{os.environ['CUDA_HOME']}/bin:\" + os.environ.get(\"PATH\", \"\")\n",
        "os.environ[\"LD_LIBRARY_PATH\"] = f\"{os.environ['CUDA_HOME']}/lib64:\" + os.environ.get(\"LD_LIBRARY_PATH\", \"\")\n"
      ],
      "metadata": {
        "id": "t1RlC8FQ9wHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#üñ•Ô∏è CUDA & GPU Verification\n",
        "- Check GPU availability and print device info."
      ],
      "metadata": {
        "id": "wcAyLNiB94rI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_gpu():\n",
        "    \"\"\"Print GPU availability and device details.\"\"\"\n",
        "    print(\"CUDA available:\", torch.cuda.is_available())\n",
        "    print(\"CUDA device count:\", torch.cuda.device_count())\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"Device name:\", torch.cuda.get_device_name(0))\n",
        "    else:\n",
        "        print(\"No GPU found.\")\n",
        "\n",
        "# Run the check\n",
        "check_gpu()\n",
        "\n",
        "# Confirm nvcc installation and version\n",
        "!nvcc --version\n"
      ],
      "metadata": {
        "id": "HMmL57GL98Rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#üõ†Ô∏è DeepSpeed Build Configuration\n",
        "- Configure environment variables and install DeepSpeed."
      ],
      "metadata": {
        "id": "e-Lr3rjr-Ahl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prevent DeepSpeed from rebuilding ops if CUDA toolkit not found\n",
        "export DS_BUILD_OPS=0\n",
        "export CUDA_HOME=/usr\n",
        "\n",
        "# Install DeepSpeed cleanly\n",
        "!DS_BUILD_AIO=0 pip install deepspeed --no-cache-dir --force-reinstall\n"
      ],
      "metadata": {
        "id": "q3sBbZ9r-Chf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#üîê Hugging Face Authentication\n",
        "- Log in to Hugging Face to enable dataset/model pulls and pushes."
      ],
      "metadata": {
        "id": "QXHPX1Gm-EF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hf_login(token: str):\n",
        "    \"\"\"\n",
        "    Authenticate with the Hugging Face Hub.\n",
        "    Replace 'YOUR_TOKEN' with your actual token.\n",
        "    \"\"\"\n",
        "    login(token=token)\n",
        "    print(\"Logged into Hugging Face as:\", importlib.metadata.version(\"transformers\"))\n",
        "\n",
        "hf_login(token=\"YOUR_HF_TOKEN\")\n"
      ],
      "metadata": {
        "id": "DuNJxteI-IbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ü§ñ Model & Tokenizer Initialization\n",
        "- Load model/tokenizer, set up LoRA configuration."
      ],
      "metadata": {
        "id": "g-ZUNKGJ-KsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelLoader:\n",
        "    \"\"\"Handles loading of model and tokenizer, and wraps with LoRA.\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str):\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "\n",
        "    def load_tokenizer(self):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "        # Ensure pad token is defined\n",
        "        if self.tokenizer.pad_token_id is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
        "\n",
        "    def load_model(self, dtype=torch.float16):\n",
        "        # Load in half precision for GPU memory savings\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_name, torch_dtype=dtype\n",
        "        )\n",
        "\n",
        "    def apply_lora(self, r=16, alpha=32, dropout=0.05):\n",
        "        \"\"\"Wrap the base model with LoRA adapters.\"\"\"\n",
        "        lora_cfg = LoraConfig(\n",
        "            r=r, lora_alpha=alpha, lora_dropout=dropout,\n",
        "            bias=\"none\", task_type=\"CAUSAL_LM\",\n",
        "            target_modules=[\"q_proj\", \"v_proj\"]\n",
        "        )\n",
        "        self.model = get_peft_model(self.model, lora_cfg)\n",
        "        self.model.print_trainable_parameters()  # sanity check\n",
        "\n",
        "# Usage\n",
        "loader = ModelLoader(\"meta-llama/Llama-3.2-1B\")\n",
        "loader.load_tokenizer()\n",
        "loader.load_model()\n",
        "loader.apply_lora()\n",
        "tokenizer, model = loader.tokenizer, loader.model\n"
      ],
      "metadata": {
        "id": "X8FLxxmA-mHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#üìù Data Preprocessing Function\n",
        "- Define a function to convert raw examples into training features."
      ],
      "metadata": {
        "id": "OIhUOCOL-puo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sample(example, tokenizer, max_len=1024):\n",
        "    \"\"\"\n",
        "    Tokenize and mask input so that loss is only computed on the assistant's output.\n",
        "    \"\"\"\n",
        "    user = example[\"user\"].strip()\n",
        "    assistant = example[\"assistant\"].strip()\n",
        "    reasoning = example.get(\"reasoning\", \"\").strip()\n",
        "\n",
        "    # Combine assistant answer + chain-of-thought if present\n",
        "    full_ans = f\"{assistant}\\nReasoning: {reasoning}\" if reasoning else assistant\n",
        "    prompt = f\"### User:\\n{user}\\n\\n### Assistant:\\n\"\n",
        "\n",
        "    # Tokenize\n",
        "    enc = tokenizer(prompt + full_ans,\n",
        "                    max_length=max_len, padding=\"max_length\",\n",
        "                    truncation=True, return_tensors=\"pt\")\n",
        "    inp_ids = enc.input_ids[0]\n",
        "    attn = enc.attention_mask[0]\n",
        "\n",
        "    # Determine prompt length to mask labels\n",
        "    prompt_ids = tokenizer(prompt,\n",
        "                           max_length=max_len, padding=\"max_length\",\n",
        "                           truncation=True, return_tensors=\"pt\")\n",
        "    prompt_len = prompt_ids.attention_mask[0].sum().item()\n",
        "\n",
        "    labels = inp_ids.clone()\n",
        "    labels[:prompt_len] = -100          # do not compute loss on prompt\n",
        "    labels[attn == 0] = -100            # ignore padding\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": inp_ids.tolist(),\n",
        "        \"attention_mask\": attn.tolist(),\n",
        "        \"labels\": labels.tolist(),\n",
        "    }\n",
        "\n",
        "# Example mapping\n",
        "raw_ds = load_dataset(\"KingNish/reasoning-base-20k\", split=\"train\")\n",
        "processed_ds = raw_ds.map(\n",
        "    lambda ex: preprocess_sample(ex, tokenizer),\n",
        "    remove_columns=raw_ds.column_names,\n",
        "    batched=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "yg5k6ybv-rec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#üöÄ Training Pipeline Class\n",
        "- Encapsulate dataset split, Trainer setup, and training logic."
      ],
      "metadata": {
        "id": "u4NTccxi-v2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LoraTrainer:\n",
        "    \"\"\"Orchestrates the fine-tuning process with Hugging Face Trainer.\"\"\"\n",
        "\n",
        "    def __init__(self, model, tokenizer, dataset, output_dir=\"output\", epochs=3):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.raw_ds = dataset\n",
        "        self.output_dir = output_dir\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def prepare_datasets(self, test_size=0.1):\n",
        "        split = self.raw_ds.train_test_split(test_size=test_size)\n",
        "        self.train_ds = split[\"train\"].map(\n",
        "            lambda ex: preprocess_sample(ex, self.tokenizer),\n",
        "            remove_columns=split[\"train\"].column_names, batched=False\n",
        "        )\n",
        "        self.eval_ds = split[\"test\"].map(\n",
        "            lambda ex: preprocess_sample(ex, self.tokenizer),\n",
        "            remove_columns=split[\"test\"].column_names, batched=False\n",
        "        )\n",
        "\n",
        "    def train(self, bs=2, grad_acc=8):\n",
        "        args = TrainingArguments(\n",
        "            output_dir=self.output_dir,\n",
        "            overwrite_output_dir=True,\n",
        "            num_train_epochs=self.epochs,\n",
        "            per_device_train_batch_size=bs,\n",
        "            gradient_accumulation_steps=grad_acc,\n",
        "            fp16=True,\n",
        "            save_strategy=\"epoch\",\n",
        "            deepspeed=\"config.json\",\n",
        "            logging_steps=100,\n",
        "            report_to=\"none\",\n",
        "            remove_unused_columns=False,\n",
        "            seed=42\n",
        "        )\n",
        "        trainer = Trainer(\n",
        "            model=self.model,\n",
        "            tokenizer=self.tokenizer,\n",
        "            args=args,\n",
        "            train_dataset=self.train_ds,\n",
        "            eval_dataset=self.eval_ds\n",
        "        )\n",
        "        trainer.train()\n",
        "        return trainer\n",
        "\n",
        "# Run training\n",
        "trainer_obj = LoraTrainer(model, tokenizer, raw_ds)\n",
        "trainer_obj.prepare_datasets()\n",
        "trainer = trainer_obj.train()\n"
      ],
      "metadata": {
        "id": "kWl9yIX3-xto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#üíæ Merge LoRA Weights & Save Full Model\n",
        "- After training, merge adapters and export the complete model."
      ],
      "metadata": {
        "id": "CDT1nluv-2jZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_and_save(trainer, save_dir=\"lora-merged-model\"):\n",
        "    \"\"\"\n",
        "    Merge LoRA weights into the base model and save tokenizer + model.\n",
        "    \"\"\"\n",
        "    print(\"Merging LoRA adapters into base model...\")\n",
        "    base = trainer.model.merge_and_unload()\n",
        "    base.eval()\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    base.save_pretrained(save_dir)\n",
        "    trainer.tokenizer.save_pretrained(save_dir)\n",
        "    print(f\"Model saved to {save_dir}\")\n",
        "\n",
        "merge_and_save(trainer, save_dir=\"llama-1b-finetuned-full\")\n"
      ],
      "metadata": {
        "id": "vj6c7HNH-4w2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#üß™ Inference Example\n",
        "- Load the merged model and run a sample prompt."
      ],
      "metadata": {
        "id": "stVTjSft-6P_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_inference(model_dir, prompt_text, max_tokens=200):\n",
        "    \"\"\"\n",
        "    Load a saved model and tokenizer, then generate a response.\n",
        "    \"\"\"\n",
        "    tok = AutoTokenizer.from_pretrained(model_dir)\n",
        "    mdl = AutoModelForCausalLM.from_pretrained(\n",
        "        model_dir, torch_dtype=torch.float16, device_map=\"auto\"\n",
        "    )\n",
        "    mdl.eval()\n",
        "\n",
        "    system = (\n",
        "        \"You are a helpful assistant. Answer concisely and accurately. \"\n",
        "        \"If unsure, say you don't know.\"\n",
        "    )\n",
        "    full_prompt = f\"{system}\\n\\n### User:\\n{prompt_text}\\n\\n### Assistant:\\n\"\n",
        "    inputs = tok(full_prompt, return_tensors=\"pt\").to(mdl.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = mdl.generate(\n",
        "            **inputs, max_new_tokens=max_tokens,\n",
        "            temperature=0.1, top_p=0.9, top_k=50,\n",
        "            repetition_penalty=1.1, no_repeat_ngram_size=1\n",
        "        )\n",
        "    text = tok.decode(out[0], skip_special_tokens=True)\n",
        "    answer = text.split(\"### Assistant:\")[-1].strip()\n",
        "    print(\"üß† Model Reply:\\n\", answer)\n",
        "\n",
        "# Example inference\n",
        "run_inference(\n",
        "    model_dir=\"llama-1b-finetuned-full\",\n",
        "    prompt_text=\"What is the discriminant of the quadratic equation 5x^2 - 2x + 1 = 0?\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "wGKWBMZR--P4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}